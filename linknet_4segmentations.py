# -*- coding: utf-8 -*-
"""LINKNET_4Segmentations.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MIrx2KIKrvlPGexk_3a40XkuJESJxjTR
"""

from google.colab import drive
drive.mount('/content/drive', force_remount=True)

import os
pathRaiz='/content/drive/MyDrive/4seg_teste/'
os.chdir(pathRaiz)

#img_list = os.listdir("/content/drive/MyDrive/4seg_teste");

!git clone https://github.com/Tessellate-Imaging/Monk_Object_Detection.git

!cd /content/drive/MyDrive/4seg_teste/Monk_Object_Detection/9_segmentation_models/installation && cat requirements_colab.txt | xargs -n 1 -L 1 pip install

import os
import sys
sys.path.append("content/drive/MyDrive/4seg_teste/Monk_Object_Detection/9_segmentation_models/lib/");

"""testes para resolver o problema infer_segmentations"""

!pip install inference-tools

!pip install keras
!pip install patchify    #To install and import other mentioned libraries  in code
!pip install segmentation_models

!pip install tf-models-official
!pip install tf-models-nightly
!pip install tensorflow-text
!pip install tensorflow-text-nightly
!pip install tf-models-no-deps

!pip install kaggle

!pip install segmentation-models
!pip install git+https://github.com/qubvel/segmentation_models

pip install segmentation

import os
import cv2
import numpy as np
from tqdm import tqdm

import os
import glob
import numpy as np
from tensorflow import keras
import tensorflow as tf

"""fim dos testes do infer_segmentation"""

from infer_segmentation import Infer

tf = Infer();

tf = Segmenter();
#gtf = Segmenter();

classes_dict = {
    'background': 0,
    'doenca': 1
};
classes_to_train = ['background', 'doenca'];

tf.Data_Params(classes_dict, classes_to_train, image_shape=[400, 225])

!wget --load-cookies /tmp/cookies.txt "https://docs.google.com/uc?export=download&confirm=$(wget --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1FbPdoKbPn52VcmVJFRbdjD_pxcj8kWWF' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\1\n/p')&id=1FbPdoKbPn52VcmVJFRbdjD_pxcj8kWWF" -O seg_water_trained.zip && rm -rf /tmp/cookies.txt

train_img_dir = "/content/drive/MyDrive/teste/root_img_dir/train_img_dir/";
train_mask_dir = "/content/drive/MyDrive/teste/root_img_dir/train_mask_dir/";

val_img_dir = "/content/drive/MyDrive/teste/root_img_dir/val_img_dir/";
val_mask_dir = "/content/drive/MyDrive/teste/root_img_dir/val_mask_dir/";

tf.Train_Dataset(train_img_dir, train_mask_dir, classes_dict, classes_to_train)
#gtf.Train_Dataset(train_img_dir, train_mask_dir, classes_dict, classes_to_train)

tf.Val_Dataset(val_img_dir, val_mask_dir)
#gtf.Val_Dataset(val_img_dir, val_mask_dir)

#gtf.List_Backbones();
tf.List_Backbones();

tf.Data_Params(batch_size=2, backbone="efficientnetb3", image_shape=[400, 225])
#gtf.Data_Params(batch_size=2, backbone="efficientnetb3", image_shape=[400, 225])

tf.List_Models();
#gtf.List_Models();

tf.Model_Params(model="Unet")
#gtf.Model_Params(model="Unet")

#gtf.Train_Params(lr=0.0001)
tf.Train_Params(lr=0.0001)

import keras
from keras.models import Sequential, Model
from keras.layers import Dense, Flatten, Conv3D, MaxPooling3D, Dropout, BatchNormalization, LeakyReLU
from keras.regularizers import l2

#import tensorflow.compat.v1 as tf
#tf.get_default_graph()"
#tf.compat.v1.get_default_graph()

import tensorflow as tf
from tensorflow import keras

tf.Setup();
#gtf.Setup();

tf.Train(num_epochs=300);
#gtf.Train(num_epochs=300);

tf.Visualize_Training_History();

tf.Model_Params(model="Unet", backbone="efficientnetb7", path_to_model='/content/drive/MyDrive/teste/root_img_dir/seg_water_trained/best_model.h5')